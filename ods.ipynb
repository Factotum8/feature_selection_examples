{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Построение и отбор признаков](https://habr.com/ru/company/ods/blog/325422/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сте́мминг** — это процесс нахождения основы слова для заданного исходного слова. Основа слова не обязательно совпадает с морфологическим корнем слова.  \n",
    "**Лемматиза́ция** — процесс приведения словоформы к лемме — её нормальной (словарной) форме. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# перед началом работы не забудьте скачать файл train.json.zip с Kaggle и разархивировать его\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# сразу загрузим датасет от Renthop\n",
    "with open('./data/train.json', 'r') as raw_data:\n",
    "    data = json.load(raw_data)\n",
    "    df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 0. 1. 1. 0. 0.]\n",
      "[0. 1. 1. 1. 0. 0. 1.]\n",
      "[1. 1. 1. 2. 1. 2. 1.]\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "import numpy as np\n",
    "\n",
    "texts = [['i', 'have', 'a', 'cat'],\n",
    "         ['he', 'have', 'a', 'dog'],\n",
    "         ['he', 'and', 'i', 'have', 'a', 'cat', 'and', 'a', 'dog']]\n",
    "\n",
    "dictionary = list(enumerate(set(reduce(lambda x, y: x + y, texts))))\n",
    "\n",
    "def vectorize(text):\n",
    "    vector = np.zeros(len(dictionary))\n",
    "    for i, word in dictionary:\n",
    "        num = 0\n",
    "        for w in text:\n",
    "            if w == word:\n",
    "                num += 1\n",
    "        if num:\n",
    "            vector[i] = num\n",
    "    return vector\n",
    "\n",
    "for t in texts:\n",
    "    print(vectorize(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'no': 2, 'have': 1, 'cows': 0}\n",
      "{'no': 4, 'have': 1, 'cows': 0, 'no have': 6, 'have cows': 2, 'have no': 3, 'no cows': 5}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer(ngram_range=(1,1)) \n",
    "\n",
    "vect.fit_transform(['no i have cows', 'i have no cows']).toarray()\n",
    "print(vect.vocabulary_)\n",
    "vect = CountVectorizer(ngram_range=(1,2)) \n",
    "\n",
    "vect.fit_transform(['no i have cows', 'i have no cows']).toarray()\n",
    "print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Шпаргалка по CountVectorizer](http://zabaykin.ru/?p=463)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: ['слово1', 'слово2', 'слово3', 'слово4']\n",
      "Get index: 2\n",
      "All index: {'слово1': 0, 'слово2': 1, 'слово3': 2, 'слово4': 3}\n",
      "Matrix: [[1 1 1 0]\n",
      " [0 1 1 0]\n",
      " [2 1 0 0]\n",
      " [0 0 0 1]]\n",
      "New doc:\n",
      "  (0, 0)\t1\n",
      "  (0, 3)\t2\n",
      "Matrix frequency: [3 3 2 1]\n",
      "Final matrix:\n",
      "[['слово1' 'слово2' 'слово3' 'слово4']\n",
      " ['3' '3' '2' '1']]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    " \n",
    "# инициализируем\n",
    "vectorizer = CountVectorizer()\n",
    "# составляем корпус документов\n",
    "corpus = [\n",
    "  'слово1 слово2 слово3',\n",
    "  'слово2 слово3',\n",
    "  'слово1 слово2 слово1',\n",
    "  'слово4'\n",
    "]\n",
    "# подсчитываем\n",
    "X = vectorizer.fit_transform(corpus)\n",
    " \n",
    "# таким образом будет подсчитана следующая структура:\n",
    "#        | слово1 | слово2 | слово3 | слово4\n",
    "# текст1 |   1    |    1   |   1    |   0\n",
    "# текст2 |   0    |    1   |   1    |   0\n",
    "# текст3 |   2    |    1   |   0    |   0\n",
    "# текст4 |   0    |    0   |   0    |   1\n",
    " \n",
    "# чтобы получить сгенерированный словарь, из приведенной структуры CountVectorizer, \n",
    "# стоит отметить что порядок совпадает с матрицей\n",
    "print(f\"Words: {vectorizer.get_feature_names()}\")  # ['слово1', 'слово2', 'слово3', 'слово4']\n",
    " \n",
    "# чтобы узнать индекс токена в словаре\n",
    "print(f\"Get index: {vectorizer.vocabulary_.get('слово3')}\") # вернет 2\n",
    "print(f\"All index: {vectorizer.vocabulary_}\")\n",
    "\n",
    "      # показать матрицу\n",
    "print(f\"Matrix: {X.toarray()}\")\n",
    "\n",
    "# теперь можно быстро подсчитать вектор для нового документа\n",
    "print(f\"New doc:\\n{vectorizer.transform(['слово1 слово4 слово4'])}\")\n",
    " \n",
    "# чтобы узнать количественное вхождение каждого слова:\n",
    "matrix_freq = np.asarray(X.sum(axis=0)).ravel()\n",
    "# ravel сделать одномерным\n",
    "print(f\"Matrix frequency: {matrix_freq}\")\n",
    "\n",
    "final_matrix = np.array([np.array(vectorizer.get_feature_names()), matrix_freq])\n",
    "print(f\"Final matrix:\\n{final_matrix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также отмечу, что необязательно оперировать именно словами: в некоторых случаях можно генерировать N-граммы из букв (например, такой алгоритм учтет сходство родственных слов или опечаток)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1622776601683795\n",
      "2.8284271247461903\n",
      "3.4641016151377544\n",
      "3.7416573867739413\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{' ив': 1,\n",
       " 'ива': 12,\n",
       " 'ван': 8,\n",
       " 'ано': 6,\n",
       " 'нов': 18,\n",
       " 'ов ': 19,\n",
       " ' пе': 2,\n",
       " 'пет': 20,\n",
       " 'етр': 10,\n",
       " 'тро': 25,\n",
       " 'ров': 22,\n",
       " 'тре': 24,\n",
       " 'рен': 21,\n",
       " 'енк': 9,\n",
       " 'нко': 17,\n",
       " 'ко ': 14,\n",
       " ' см': 3,\n",
       " 'сми': 23,\n",
       " 'мит': 16,\n",
       " 'ит ': 13,\n",
       " ' аз': 0,\n",
       " 'аза': 4,\n",
       " 'зам': 11,\n",
       " 'ама': 5,\n",
       " 'мат': 15,\n",
       " 'ат ': 7}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "vect = CountVectorizer(ngram_range=(3,3), analyzer='char_wb') \n",
    "\n",
    "n1, n2, n3, n4, n5 = vect.fit_transform(\n",
    "    ['иванов', 'петров', 'петренко', 'смит', 'азамат']).toarray()\n",
    "\n",
    "print(euclidean(n1, n2))  # Евклидово растояние \n",
    "print(euclidean(n2, n3))\n",
    "print(euclidean(n3, n4))\n",
    "print(euclidean(n3, n5))\n",
    "\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто для задач, связанных с картинками, используется какая-нибудь сверточная сеть. Можно не придумывать архитектуру и не обучать сеть с нуля, а взять предобученную state of the art сеть, веса которой можно скачать из открытых источников. Чтобы адаптировать ее под свою задачу, дата сайнтисты практикуют т.н. fine tuning: последние полносвязные слои сети \"отрываются\", вместо них добавляются новые, подобранные под конкретную задачу, и сеть дообучается на новых данных. Но если вы хотите просто векторизовать изображение для каких-то своих целей (например, использовать какой-то несетевой классификатор) – просто оторвите последние слои и используйте выход предыдущих слоев:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 1024, 3)\n",
      "(224, 224, 3)\n",
      "(1, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from scipy.misc import face\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f, ax = plt.subplots(nrows=2, ncols=1)\n",
    "ax[0].axis('off')\n",
    "ax[1].axis('off')\n",
    "\n",
    "print(face().shape)\n",
    "img = image.array_to_img(face())\n",
    "ax[0].imshow(img)\n",
    "img = img.resize((224, 224))\n",
    "ax[1].imshow(img)\n",
    "x = image.img_to_array(img)\n",
    "print(x.shape)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/worker/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/worker/anaconda3/lib/python3.7/site-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    }
   ],
   "source": [
    "resnet_settings = {'include_top': False, 'weights': 'imagenet'}\n",
    "resnet = ResNet50(**resnet_settings)\n",
    "\n",
    "img = image.array_to_img(face())\n",
    "# какой милый енот! \n",
    "img = img.resize((224, 224))\n",
    "# в реальной жизни может понадобиться внимательнее относиться к ресайзу\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "# нужно дополнительное измерение, т.к. модель рассчитана на работу с массивом изображений\n",
    "\n",
    "features = resnet.predict(x)  # не очень понял чего это"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прочитать текст на картинке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TesseractNotFoundError",
     "evalue": "tesseract is not installed or it's not in your path",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pytesseract/pytesseract.py\u001b[0m in \u001b[0;36mrun_tesseract\u001b[0;34m(input_filename, output_filename_base, extension, lang, config, nice)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msubprocess_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[1;32m    774\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    776\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1521\u001b[0m                             \u001b[0merr_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1522\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1523\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'tesseract': 'tesseract'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTesseractNotFoundError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-57644b058c57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/1.png'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m#     img = Image.open(BytesIO(img.content))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpytesseract\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pytesseract/pytesseract.py\u001b[0m in \u001b[0;36mimage_to_string\u001b[0;34m(image, lang, config, nice, boxes, output_type)\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrun_and_get_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pytesseract/pytesseract.py\u001b[0m in \u001b[0;36mrun_and_get_output\u001b[0;34m(image, extension, lang, config, nice, return_bytes)\u001b[0m\n\u001b[1;32m    200\u001b[0m         }\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mrun_tesseract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'output_filename_base'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextsep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pytesseract/pytesseract.py\u001b[0m in \u001b[0;36mrun_tesseract\u001b[0;34m(input_filename, output_filename_base, extension, lang, config, nice)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msubprocess_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTesseractNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTesseractNotFoundError\u001b[0m: tesseract is not installed or it's not in your path"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# In : img = 'http://ohscurrent.org/wp-content/uploads/2015/09/domus-01-google.jpg'\n",
    "# просто случайная картинка из поиска \n",
    "# img = requests.get(img)\n",
    "with Image.open('./data/1.png') as im:\n",
    "    #     img = Image.open(BytesIO(img.content))\n",
    "    text = pytesseract.image_to_string(img)\n",
    "    text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
